{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etKrzFB2kptw"
   },
   "source": [
    "### Install sentence transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9531,
     "status": "ok",
     "timestamp": 1612365379046,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "DTlHrOwRkxA0",
    "outputId": "8f317f19-f649-4b8b-aec7-6173418abad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/e2/84d6acfcee2d83164149778a33b6bdd1a74e1bcb59b2b2cd1b861359b339/sentence-transformers-0.4.1.2.tar.gz (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 5.4MB/s \n",
      "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8MB 9.9MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 31.5MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.8)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 48.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.8)\n",
      "Collecting tokenizers==0.9.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 51.3MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Building wheels for collected packages: sentence-transformers, sacremoses\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-0.4.1.2-cp36-none-any.whl size=103068 sha256=a37865dc327e71ad247767b601dce237c89023f053884af07c64bf67e6d90799\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/33/d1/5703dd56199c09d4a1b41e0c07fb4e7765a84d787cbdc48ac3\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=1a4285823f064643bf3cf85cdd1338285f477ca169955113bdcb1f658696ef92\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sentence-transformers sacremoses\n",
      "Installing collected packages: sacremoses, tokenizers, transformers, sentencepiece, sentence-transformers\n",
      "Successfully installed sacremoses-0.0.43 sentence-transformers-0.4.1.2 sentencepiece-0.1.95 tokenizers-0.9.4 transformers-4.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-UkyvR9egcs"
   },
   "source": [
    "### Useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33828,
     "status": "ok",
     "timestamp": 1612365403352,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "Yhr83nwuejYo",
    "outputId": "974acbce-c119-4786-841d-bc2f57d20ad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import json,glob,nltk,copy,torch,time,sentence_transformers,pickle\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from queue import PriorityQueue\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "nltk.download('punkt')\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ct20dwbsmhb"
   },
   "source": [
    "### Retrieve dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74694,
     "status": "ok",
     "timestamp": 1612365444227,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "1Rmi5RJRibuG",
    "outputId": "df7f5161-b88f-43b0-83bb-04a1a8275d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-03 15:16:42--  https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-03-13.tar.gz\n",
      "Resolving ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)... 52.218.234.233\n",
      "Connecting to ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)|52.218.234.233|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 278921140 (266M) [application/x-tar]\n",
      "Saving to: ‘cord-19_2020-03-13.tar.gz’\n",
      "\n",
      "cord-19_2020-03-13. 100%[===================>] 266.00M  61.7MB/s    in 4.6s    \n",
      "\n",
      "2021-02-03 15:16:47 (57.9 MB/s) - ‘cord-19_2020-03-13.tar.gz’ saved [278921140/278921140]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-03-13.tar.gz\n",
    "!tar -xf cord-19_2020-03-13.tar.gz\n",
    "!tar -xf 2020-03-13/comm_use_subset.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsWRMwlR6DwQ"
   },
   "source": [
    "### Prepare GPU Cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74686,
     "status": "ok",
     "timestamp": 1612365444229,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "hN25lAzQ6Jws",
    "outputId": "dca16130-2dd4-41c7-f865-48ef64e7f892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i798G_ziUFqQ"
   },
   "source": [
    "### Read JSON files and store title,abstract and text of each article into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kfpcKOZ-a6S"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "files = glob.glob('comm_use_subset/*', recursive=True)\n",
    "number_of_articles = len(files)\n",
    "bound = 9000\n",
    "\n",
    "for single_file in files[0:bound]:\n",
    "  with open(single_file, 'r') as f:\n",
    "    json_file = json.load(f)\n",
    "    \n",
    "    # Retrieve title\n",
    "    title = json_file[\"metadata\"][\"title\"]\n",
    "\n",
    "    # Retrieve abstracts\n",
    "    abstracts = []\n",
    "    if len(json_file[\"abstract\"]) != 0 :\n",
    "      for abstract in json_file[\"abstract\"]:\n",
    "        abstracts.append(abstract[\"text\"])\n",
    "\n",
    "    # Retrieve texts\n",
    "    texts = []\n",
    "    for text in json_file[\"body_text\"]:\n",
    "      texts.append(text[\"text\"])\n",
    "\n",
    "    data.append([title,abstracts,texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYl7u849jyJ2"
   },
   "source": [
    "### Convert corpus to sentences with help of library nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IY127-hMhn98"
   },
   "outputs": [],
   "source": [
    "# For each article\n",
    "for article in range(bound):\n",
    " \n",
    "  # Abstracts section \n",
    "  for abstract in range(len(data[article][1])): \n",
    "    data[article][1][abstract] = nltk.sent_tokenize(data[article][1][abstract])\n",
    "   \n",
    "  # Texts section\n",
    "  for text in range(len(data[article][2])):\n",
    "    data[article][2][text] = nltk.sent_tokenize(data[article][2][text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv3dZmeDoJIE"
   },
   "source": [
    "### Transform sentences to embeddings with first sentence trasformer 'stsb-bert-base' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6620323,
     "status": "ok",
     "timestamp": 1612372188980,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "TtobtUXBEAda",
    "outputId": "cf56da09-bcd2-4257-ab35-11d6751718ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 110.3 minutes\n"
     ]
    }
   ],
   "source": [
    "# We gonna calculate time spent for sentence embeddings\n",
    "fist_start_time = time.time()\n",
    "\n",
    "# Declare our first sentence transformer model and pass it to appropriate device\n",
    "first_model = SentenceTransformer('stsb-bert-base').to(device)\n",
    "\n",
    "# Here we'll put sentence embeddings of each article from first sentence transformer model\n",
    "first_sentence_embeddings = [[] for i in range(bound)]\n",
    "\n",
    "# Here we'll put for each article sentences of abstract and body text\n",
    "flatten_data = [[] for i in range(bound)]\n",
    "\n",
    "# For each article\n",
    "for article in range(bound):\n",
    "  \n",
    "  # Get abstract and body text of each article\n",
    "  abstract_ = data[article][1]\n",
    "  body_ = data[article][2]\n",
    "\n",
    "  # Process to keep abstract and body text's sentences of each article in a big list\n",
    "  for abstract in abstract_:\n",
    "    for sentence in abstract:\n",
    "      flatten_data[article].append(sentence)\n",
    "  for text in body_:\n",
    "    for sentence in text:\n",
    "      flatten_data[article].append(sentence)\n",
    "\n",
    "  # Transform sentences to embeddings \n",
    "  first_sentence_embeddings[article].append(first_model.encode(flatten_data[article],convert_to_tensor=True))\n",
    "\n",
    "  # Convert all sentence embeddings to a 2D pytorch tensor\n",
    "  first_sentence_embeddings[article] = torch.cat(first_sentence_embeddings[article]) \n",
    " \n",
    "# Check elapsed time of first model\n",
    "first_model_time = (time.time() - fist_start_time)/60\n",
    "print(\"Elapsed time: %s minutes\" % (round(first_model_time,1)))\n",
    "\n",
    "# # Save first sentence embeddings\n",
    "# with open('/content/drive/MyDrive/first_sentence_embeddings.pkl', 'wb') as f:\n",
    "#   pickle.dump(first_sentence_embeddings,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luXB8zb2ujm9"
   },
   "source": [
    "### Random printings just for safety reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6620314,
     "status": "ok",
     "timestamp": 1612372188983,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "rAPejYmjgGbN",
    "outputId": "fdebc945-2163-4ed8-d71f-cbebbc099592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 <class 'list'>\n",
      "140 <class 'torch.Tensor'>\n",
      "torch.Size([140, 768])\n",
      "9000 <class 'list'>\n",
      "140 <class 'list'>\n",
      "150 <class 'str'>\n",
      "The essential requirement of the lymphotoxin beta receptor (LTβR) in the development and maintenance of peripheral lymphoid organs is well recognized.\n"
     ]
    }
   ],
   "source": [
    "# Testing with prints\n",
    "print(len(first_sentence_embeddings),type(first_sentence_embeddings))\n",
    "print(len(first_sentence_embeddings[0]),type(first_sentence_embeddings[0]))\n",
    "print(first_sentence_embeddings[0].shape)\n",
    "\n",
    "print(len(flatten_data),type(flatten_data))\n",
    "print(len(flatten_data[0]),type(flatten_data[0]))\n",
    "print(len(flatten_data[0][0]),type(flatten_data[0][0]))\n",
    "print(flatten_data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8N9sewtpXLxz"
   },
   "source": [
    "### Declare our queries and tranform them to embeddings based on our two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6620311,
     "status": "ok",
     "timestamp": 1612372188984,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "rWl9zu5ix4hO",
    "outputId": "10f14f8e-8f4a-4c08-d221-95e5b8296f07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1st model... Number Of Queries: 8  Query Embedding's Length: 768\n"
     ]
    }
   ],
   "source": [
    "queries = ['What are the coronoviruses?','What was discovered in Wuhuan in December 2019?',\n",
    "           'What is Coronovirus Disease 2019?','What is COVID-19?','What is caused by SARS-COV2?',\n",
    "           'How is COVID-19 spread?','Where was COVID-19 discovered?','How does coronavirus spread?']\n",
    "\n",
    "first_queries_embeddings = first_model.encode(queries,convert_to_tensor=True)\n",
    "\n",
    "print(\"For 1st model... Number Of Queries:\",len(first_queries_embeddings),\" Query Embedding's Length:\",len(first_queries_embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCJRyPRTYa_P"
   },
   "source": [
    "### Test our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6665444,
     "status": "ok",
     "timestamp": 1612372234122,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "Sn_G7bde1YNB",
    "outputId": "3048c1fc-90f1-418e-ce85-2131bf070cf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the coronoviruses? \n",
      "\n",
      "Answer 1 : in orthomyxoviruses (e.g. \n",
      "From article: Mapping overlapping functional elements embedded within the protein-coding regions of RNA viruses \n",
      "Score: 0.7579688 \n",
      "\n",
      "Answer 2 : C) Parechoviruses. \n",
      "From article: A viral metagenomic survey identifies known and novel mammalian viruses in bats from Saudi Arabia \n",
      "Score: 0.7296622 \n",
      "\n",
      "Answer 3 : reoviruses and orthomyxoviruses). \n",
      "From article: Non-canonical translation in RNA viruses \n",
      "Score: 0.6961699 \n",
      "\n",
      "Answer 4 : Adenoviruses. \n",
      "From article: Virus-induced exacerbations in asthma and COPD \n",
      "Score: 0.6928731 \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "Query: What was discovered in Wuhuan in December 2019? \n",
      "\n",
      "Answer 1 : In December 2019 \n",
      "From article: Clinical Medicine Characteristics of and Public Health Responses to the Coronavirus Disease 2019 Outbreak in China \n",
      "Score: 0.7151003 \n",
      "\n",
      "Answer 2 : (2019c) , Zhao et al. \n",
      "From article: Distributed under Creative Commons CC-BY 4.0 Modelling the effective reproduction number of vector-borne diseases: the yellow fever outbreak in Luanda, Angola 2015-2016 as an example \n",
      "Score: 0.69906324 \n",
      "\n",
      "Answer 3 : 2019; Li et al. \n",
      "From article: Preparation and characterization of a single-domain antibody specific for the porcine epidemic diarrhea virus spike protein \n",
      "Score: 0.69682556 \n",
      "\n",
      "Answer 4 : WHO are referring to it as '2019-nCov'. \n",
      "From article: President, World Organisation of Family Doctors (WONca) \n",
      "Score: 0.63965595 \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "Query: What is Coronovirus Disease 2019? \n",
      "\n",
      "Answer 1 : The spectrum of this disease in humans, now named coronavirus disease 2019 (COVID-19) [5] , is yet to be fully determined. \n",
      "From article: Rapid communication \n",
      "Score: 0.62452435 \n",
      "\n",
      "Answer 2 : Vaccines 2019, 7, 173 2 of 12 the CV777 vaccine strain belonging to subtype G1 [11] . \n",
      "From article: Significant Interference with Porcine Epidemic Diarrhea Virus Pandemic and Classical Strain Replication in Small-Intestine Epithelial Cells Using an shRNA Expression Vector \n",
      "Score: 0.6191194 \n",
      "\n",
      "Answer 3 : The 2019 novel coronavirus (2019-nCoV), a betacoronavirus, forms a clade within the subgenus sarbecovirus of the Orthocoronavirinae subfamily [4] . \n",
      "From article: Potential Rapid Diagnostics, Vaccine and Therapeutics for 2019 Novel Coronavirus (2019-nCoV): A Systematic Review \n",
      "Score: 0.6055144 \n",
      "\n",
      "Answer 4 : The disease caused by SARS-CoV-2 was called \"coronavirus disease 2019\" (COVID-19) [7] . \n",
      "From article: Systematic Comparison of Two Animal-to-Human Transmitted Human Coronaviruses: SARS-CoV-2 and SARS-CoV \n",
      "Score: 0.60359573 \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "Query: What is COVID-19? \n",
      "\n",
      "Answer 1 : The main dataset of this study is COVID-19 dataset. \n",
      "From article: Clinical Medicine Optimization Method for Forecasting Confirmed Cases of COVID-19 in China \n",
      "Score: 0.65359545 \n",
      "\n",
      "Answer 2 : It remains to be seen if this will be the case for COVID-19 as well. \n",
      "From article: Clinical Medicine Incubation Period and Other Epidemiological Characteristics of 2019 Novel Coronavirus Infections with Right Truncation: A Statistical Analysis of Publicly Available Case Data \n",
      "Score: 0.62117475 \n",
      "\n",
      "Answer 3 : (19) <ii>. \n",
      "From article: Enteropathogen co-infection in UK cats with diarrhoea \n",
      "Score: 0.6208621 \n",
      "\n",
      "Answer 4 : This is particularly true for the COVID-19. \n",
      "From article: First two months of the 2019 Coronavirus Disease (COVID-19) epidemic in China: real- time surveillance and evaluation with a second derivative model \n",
      "Score: 0.60347044 \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "Query: What is caused by SARS-COV2? \n",
      "\n",
      "Answer 1 : Epidemiology of SARS-CoV and MERS-CoV \n",
      "From article: viruses Review From SARS to MERS, Thrusting Coronaviruses into the Spotlight \n",
      "Score: 0.67696023 \n",
      "\n",
      "Answer 2 : produced by SARS-CoV-infected cells, or whether it is a secondary effect, i.e. \n",
      "From article: Virology Journal Inhibition of cytokine gene expression and induction of chemokine genes in non-lymphatic cells infected with SARS coronavirus \n",
      "Score: 0.6758429 \n",
      "\n",
      "Answer 3 : Is there an effective specific anti-SARS-CoV-2 solution? \n",
      "From article: Systematic Comparison of Two Animal-to-Human Transmitted Human Coronaviruses: SARS-CoV-2 and SARS-CoV \n",
      "Score: 0.6730329 \n",
      "\n",
      "Answer 4 : If indeed so, could this then indicate a probable basis for the increased pathogenicity of SARS-CoV compared to HCoV-NL63? \n",
      "From article: The Coronavirus Nucleocapsid Is a Multifunctional Protein \n",
      "Score: 0.6687777 \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "Query: How is COVID-19 spread? \n",
      "\n",
      "Answer 1 : It is a time series method for forecasting the confirmed cases of the COVID-19, as given in Figure 2 . \n",
      "From article: Clinical Medicine Optimization Method for Forecasting Confirmed Cases of COVID-19 in China \n",
      "Score: 0.6681634 \n",
      "\n",
      "Answer 2 : The screening of the D. pulex genome database suggests 19 loci with VTG-like coding sequences. \n",
      "From article: Acclimatory responses of the Daphnia pulex proteome to environmental changes. II. Chronic exposure to different temperatures (10 and 20°C) mainly affects protein metabolism \n",
      "Score: 0.6484313 \n",
      "\n",
      "Answer 3 : The value of the basic reproduction number R 0 for the COVID-19 epidemic was calculated as \n",
      "From article: Clinical Medicine Real-Time Estimation of the Risk of Death from Novel Coronavirus (COVID-19) Infection: Inference Using Exported Cases \n",
      "Score: 0.62284505 \n",
      "\n",
      "Answer 4 : First, we compared the density of synonymous and nonsynonymous generegion SNPs in CEACAM/PSG genes to those of other genes on chromosome 19. \n",
      "From article: Widespread Divergence of the CEACAM/PSG Genes in Vertebrates and Humans Suggests Sensitivity to Selection \n",
      "Score: 0.6133218 \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "Query: Where was COVID-19 discovered? \n",
      "\n",
      "Answer 1 : It is a time series method for forecasting the confirmed cases of the COVID-19, as given in Figure 2 . \n",
      "From article: Clinical Medicine Optimization Method for Forecasting Confirmed Cases of COVID-19 in China \n",
      "Score: 0.6720117 \n",
      "\n",
      "Answer 2 : The value of the basic reproduction number R 0 for the COVID-19 epidemic was calculated as \n",
      "From article: Clinical Medicine Real-Time Estimation of the Risk of Death from Novel Coronavirus (COVID-19) Infection: Inference Using Exported Cases \n",
      "Score: 0.62296945 \n",
      "\n",
      "Answer 3 : It remains to be seen if this will be the case for COVID-19 as well. \n",
      "From article: Clinical Medicine Incubation Period and Other Epidemiological Characteristics of 2019 Novel Coronavirus Infections with Right Truncation: A Statistical Analysis of Publicly Available Case Data \n",
      "Score: 0.6046771 \n",
      "\n",
      "Answer 4 : The screening of the D. pulex genome database suggests 19 loci with VTG-like coding sequences. \n",
      "From article: Acclimatory responses of the Daphnia pulex proteome to environmental changes. II. Chronic exposure to different temperatures (10 and 20°C) mainly affects protein metabolism \n",
      "Score: 0.6042676 \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "Query: How does coronavirus spread? \n",
      "\n",
      "Answer 1 : Coronaviruses: what are they? \n",
      "From article: Revisiting the dangers of the coronavirus in the ophthalmology practice \n",
      "Score: 0.82212365 \n",
      "\n",
      "Answer 2 : Coronavirus. \n",
      "From article: Lactobacillus Mucosal Vaccine Vectors: Immune Responses against Bacterial and Viral Antigens \n",
      "Score: 0.7552244 \n",
      "\n",
      "Answer 3 : coronavirus. \n",
      "From article: Tropical Medicine and Infectious Disease Potential Intermediate Hosts for Coronavirus Transmission: No Evidence of Clade 2c Coronaviruses in Domestic Livestock from Ghana \n",
      "Score: 0.75522435 \n",
      "\n",
      "Answer 4 : Coronavirus. \n",
      "From article: Population genetics, community of parasites, and resistance to rodenticides in an urban brown rat (Rattus norvegicus) population \n",
      "Score: 0.75522435 \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "Elapsed time: 0.8 minutes\n"
     ]
    }
   ],
   "source": [
    "# # Load first sentence embeddings\n",
    "# with open('/content/drive/MyDrive/first_sentence_embeddings.pkl', 'rb') as f:\n",
    "#   first_sentence_embeddings = pickle.load(f)\n",
    "\n",
    "# We gonna calculate time spent for finding best answer\n",
    "fist_start_time = time.time()\n",
    "\n",
    "# For each query \n",
    "for qindex,query in enumerate(first_queries_embeddings):\n",
    "  # Declare an empty priority queue\n",
    "  answer_pq = PriorityQueue()\n",
    "  # For each article's sentence embedding\n",
    "  for index,embed in enumerate(first_sentence_embeddings):\n",
    "    # Find the most similar vector and return it so as to add it to priority queue\n",
    "    first_results = sentence_transformers.util.semantic_search(query,embed,top_k=1)\n",
    "    for res in first_results:\n",
    "      # Add to priority queue triple value of (score,article's index,sentence's index)\n",
    "      answer_pq.put((-res[0]['score'],index,res[0]['corpus_id']))\n",
    "  # Get vectors with the best cosine similarity (which are our answers)\n",
    "  print(\"Query:\",queries[qindex],\"\\n\")\n",
    "  for idx in range(4):  \n",
    "    res = answer_pq.get()\n",
    "    print(\"Answer\",idx+1,\":\",flatten_data[res[1]][res[2]],\"\\nFrom article:\",data[res[1]][0],\"\\nScore:\",-res[0],\"\\n\")\n",
    "  print(\"------------------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "first_model_time = (time.time() - fist_start_time)/60\n",
    "print(\"Elapsed time: %s minutes\" % (round(first_model_time,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjMTR8D7MwCh"
   },
   "source": [
    "### Notes & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4pQJW8Hx9Ve"
   },
   "source": [
    ">Note that our dataset is the initial-first release of CORD-19 dataset, 2020-03-13, which is the smallest dataset with 9000 articles. \n",
    "You can find it here: [CORD-19_Releases](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGTQQGZqM7Q5"
   },
   "source": [
    "#### Just a recap of what we presented in this notebook!\n",
    "\n",
    "\n",
    ">If we could briefly refer this notebook's approach we would say that we followed a brute force approach.So let us be more detailed...   \n",
    "1. Initially, we discover what does each json file includes and we decided to retrieve as much as possible information for each \n",
    "article. So we obtained title,abstract and body text section of each article and store them to list.\n",
    "2. Right after we did the process of corpus's tokenization to sentences. We continued by transforming all sentences of each abstract \n",
    "and body text section of each article to sentence embeddings. Here the obvious question arises... Why we didn't transform title \n",
    "section? We did experiment with title section, but we observed that at most of times there isn't so much information which pairs or\n",
    "matches with our questions. Note that all embeddings have been converted to pytorch tensors!\n",
    "3. After transforming queries to embeddings, we compare each sentence's embedding of each abstract's and body text's section of each \n",
    "article with each every query embedding, via function sentence_transformers.util.semantic_search, which innerly utilizes cosine \n",
    "similarity. -Basically, this function retrieves from us many sentences embeddings and a single query embedding and returns as topk \n",
    "best (closest by cosine similarity) sentence embeddings-. So we can understand that we search everything in order to find the most \n",
    "appropriate answer!\n",
    "4. Here we follow a more brute force way, in order to search every possible information from articles, but that \n",
    "cost us to time, approximately whole process endured 2 hours, with total use of Cuda GPU. In addition, just to remind in \n",
    "this notebook we used pretrained sbert model,'stsb-bert-base', in order to transform sentences to embeddings of size 768. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcfeqcxzBjUn"
   },
   "source": [
    "#### Notes about our results!\n",
    "\n",
    ">Our first thought before we even begin was that with our brute force approach, we may be more time and computer power consuming, but \n",
    "we are going to retrieve sentences with the best cosine similarities that we could have found. But, something that we came up was \n",
    "that cosine similarity is a metric which compares angle of two vectors and returns a number between 0 and 1 which indicates how much \n",
    "similar those two vectors are. This leads us to the result that in our problem, in order to be more similar two embeddings, they \n",
    "must have as much as possible same words (between a query and an answer). So, we can conclude that this will have a glance of \n",
    "textual similarity rather than question-answering. And from theory's belief we can prove that practically our results!\n",
    "\n",
    ">For example, in query: What are the coronoviruses?\n",
    "\n",
    ">We gain answers like: \n",
    ">1. \"in orthomyxoviruses (e.g.\"\n",
    ">2. \"C) Parechoviruses.\"\n",
    ">3. \"reoviruses and orthomyxoviruses).\"  \n",
    ">4. \"Adenoviruses.\"\n",
    "\n",
    ">We can obviously notice that all 4 answers, which gave cosine similarity approximately 0.69-0.75, have as a part of their answer \n",
    "the subword \"viruses\". And that happened because it indeed exists in our query, in word \"corono\"-\"viruses\". From that simple \n",
    "example, we can figure out that instead of an answer like \"Coronoviruses are ...\", we obtained as answer something that it is like \n",
    "or similar some words of query and in addition to that answers contain few words. This phenomenon happens at most times, because \n",
    "the less words a sentence have the less compares are gonna take place with word \"viruses\". So let's take for example two simple \n",
    "answers:\n",
    "\n",
    ">>a) \"Adenoviruses.\"\n",
    "\n",
    ">>b) \"Coronaviruses are a large family of viruses that are known to cause illness ranging from the common cold.\"\n",
    "\n",
    ">Note that second answer is the first answer we found in Google, so it's a gold answer! But unfortunately if we compare cosine \n",
    "similarity scores between our query and the above answers, first answer will 'win'. This can be explained by the fact that sentence \n",
    "embedding of second answer isn't so close to our query,for the reason that sentence embedding of \"Adenoviruses\" is closer in vector\n",
    "space than embedding of sentence with too 'confused' or seemingly 'unrelated' words. \n",
    "\n",
    ">Unfortunately, at most of our questions we can observe the exact same behavior! Another example, which can prove that is:\n",
    "\n",
    ">>Query: How does coronavirus spread?\n",
    "\n",
    ">>Answers: \n",
    ">>1. \"Coronaviruses: what are they?\"\n",
    ">>2. \"Coronavirus.\"\n",
    ">>3. \"coronavirus.\"\n",
    ">>4. \"Coronavirus.\"\n",
    "\n",
    ">>With cosine similarity scores between 0.75-0.82.\n",
    "\n",
    ">But by observing thoroughly our questions-answers we found some exceptions, like this example:\n",
    "\n",
    ">>Query: What is Coronovirus Disease 2019? \n",
    "\n",
    ">>Answers: \n",
    ">>1. \"The spectrum of this disease in humans, now named coronavirus disease 2019 (COVID-19) [5] , is yet to be fully determined.\"\n",
    ">>4. \"The disease caused by SARS-CoV-2 was called \"coronavirus disease 2019\" (COVID-19) [7] .\"\n",
    "With cosine similarity scores between 0.60-0.62. \n",
    "\n",
    ">Compared with most percentage of answers, here we have more allowable answers. Precisely fourth answer is absolutely acceptable and \n",
    "relevant, altough it has the lowest cosine similarity among the four best answers!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Task_1,2_a.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
